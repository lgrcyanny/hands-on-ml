{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple RNN Netowork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0= tf.placeholder(tf.float32, [None, n_inputs])\n",
    "x1 = tf.placeholder(tf.float32, [None, n_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))\n",
    "wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "y0 = tf.tanh(tf.matmul(x0, wx) + b)\n",
    "y1 = tf.tanh(tf.matmul(y0, wy) + tf.matmul(x1, wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mini-batch:        instance 0,instance 1,instance 2,instance 3\n",
    "x0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0\n",
    "x1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    y0_val, y1_val = sess.run([y0, y1], feed_dict={x0: x0_batch, x1: x1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.93580914 -0.87305528  0.99955434 -0.81581593 -0.93684173]\n",
      " [-0.96607804 -0.99999994  1.         -0.99948537 -1.        ]\n",
      " [-0.98220491 -1.          1.         -0.99999875 -1.        ]\n",
      " [ 0.78392994 -1.         -0.99699605 -0.99999946 -1.        ]]\n",
      "[[ 0.93722236 -1.          1.         -1.         -1.        ]\n",
      " [ 0.74702108  0.65820336 -0.44420493 -0.87313861 -0.99701184]\n",
      " [ 0.96467161 -1.          0.99999952 -0.99999183 -1.        ]\n",
      " [ 0.99798864 -0.99999952  0.99927539  0.81625563 -0.99995649]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y0_val)\n",
    "print(y1_val)\n",
    "y0_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tensorflow API\n",
    "with tf.variable_scope(\"basic_rnn\"):\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "    output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [x0, x1],\n",
    "                                                    dtype=tf.float32)\n",
    "    y0, y1 = output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 4]\n",
      " [6 5]]\n",
      "[array([1, 2, 6], dtype=int32), array([3, 4, 5], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x = tf.constant([[1, 2, 6], [3, 4, 5]])\n",
    "    y = tf.transpose(x, perm=[1, 0])\n",
    "    print sess.run(y)\n",
    "    print sess.run(tf.unstack(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"basic_rnn_with_transpose\", reuse=tf.AUTO_REUSE):\n",
    "    n_steps = 2\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "    output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, X_seqs,\n",
    "                                                    dtype=tf.float32)\n",
    "    outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.7700749   0.92492568  0.55726224 -0.12563048  0.88938397]\n",
      "  [ 1.          0.99857748  0.99993598  0.98880398  0.99998587]]\n",
      "\n",
      " [[ 0.99976492  0.9967165   0.98538113  0.65638256  0.99938405]\n",
      "  [ 0.80900317  0.16828156  0.46891025  0.12967369 -0.42948887]]\n",
      "\n",
      " [[ 0.99999982  0.9998613   0.99961859  0.93530399  0.99999684]\n",
      "  [ 0.99999839  0.92492437  0.99839371  0.9787792   0.99813163]]\n",
      "\n",
      " [[ 0.99771386 -0.99951416  0.81172067  0.99875546 -0.7823742 ]\n",
      "  [ 0.94335043 -0.69235611  0.65002167  0.90329885  0.723634  ]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope(\"basic_rnn_with_transpose\"):\n",
    "        X_batch = np.array([\n",
    "             # t = 0     t = 1\n",
    "            [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "            [[3, 4, 5], [0, 0, 0]], # instance 1\n",
    "            [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "            [[9, 0, 1], [3, 2, 1]], # instance 3\n",
    "        ])\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run()\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_batch})\n",
    "        print(outputs_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn_mnist\"):\n",
    "    n_steps = 28\n",
    "    n_inputs = 28\n",
    "    n_neurons = 150\n",
    "    n_outputs = 10\n",
    "\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(states, n_outputs)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/tensorflow\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('acc_train:', 0.84666669, 'acc_test:', 0.91100001)\n",
      "('acc_train:', 0.97333336, 'acc_test:', 0.94800001)\n",
      "('acc_train:', 0.95333332, 'acc_test:', 0.94950002)\n",
      "('acc_train:', 0.95333332, 'acc_test:', 0.95880002)\n",
      "('acc_train:', 0.97333336, 'acc_test:', 0.9637)\n",
      "('acc_train:', 0.97333336, 'acc_test:', 0.96569997)\n",
      "('acc_train:', 0.95333332, 'acc_test:', 0.96509999)\n",
      "('acc_train:', 0.96666664, 'acc_test:', 0.9716)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.96710002)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.9738)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97689998)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.9734)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97659999)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.97439998)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97310001)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.977)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97359997)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97320002)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97409999)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.97589999)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.9763)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97329998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.9752)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97259998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97259998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97409999)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.9752)\n",
      "('acc_train:', 0.97333336, 'acc_test:', 0.97750002)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97490001)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97100002)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97299999)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97689998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.9763)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97399998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97759998)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.9734)\n",
      "('acc_train:', 0.97333336, 'acc_test:', 0.97970003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97909999)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97229999)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97799999)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97600001)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97710001)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97860003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97799999)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.9788)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97829998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97860003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97280002)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97509998)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97649997)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.9774)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97320002)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97430003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97430003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.98009998)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97729999)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.98040003)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97509998)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.9777)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97539997)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.9788)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97610003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.9799)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97509998)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.97289997)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97509998)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97729999)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97420001)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97670001)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97479999)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.97640002)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.9745)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97970003)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97850001)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.9752)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97799999)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97780001)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97509998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97689998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97869998)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.98019999)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.97610003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.96899998)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.9756)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97409999)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97539997)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97549999)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.98040003)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.98210001)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97799999)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97710001)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.9763)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.97369999)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.97490001)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97500002)\n",
      "('acc_train:', 1.0, 'acc_test:', 0.9781)\n",
      "('acc_train:', 0.98000002, 'acc_test:', 0.97070003)\n",
      "('acc_train:', 0.99333334, 'acc_test:', 0.97610003)\n",
      "('acc_train:', 0.97333336, 'acc_test:', 0.97619998)\n",
      "('acc_train:', 0.98666668, 'acc_test:', 0.97549999)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"rnn_mnist\"):\n",
    "    n_epochs = 100\n",
    "    batch_size = 150\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(mnist.train.num_examples // batch_size):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_train = sess.run(accuracy, feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = sess.run(accuracy, feed_dict={X: X_test, y: y_test})\n",
    "            print(\"acc_train:\", acc_train, \"acc_test:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for predict Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"time_series\", reuse=tf.AUTO_REUSE):\n",
    "    n_steps = 20\n",
    "    n_inputs = 1\n",
    "    n_neurons = 100\n",
    "    n_outputs = 1\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "    cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "        output_size = n_outputs\n",
    "    )\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    # define cost function\n",
    "    learning_rate = 0.001\n",
    "    loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    # define execution\n",
    "    n_iterations = 100\n",
    "    batch_size = 50\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "#         for iteration in range(n_iterations):\n",
    "#             X_batch, y_batch = [...]  # fetch the next training batch\n",
    "#             sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "#             if iteration % 100 == 0:\n",
    "#                 mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "#                 print(iteration, \"\\tMSE:\", mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"deep_rnn\"):\n",
    "    n_neurons = 100\n",
    "    n_layers = 3\n",
    "    layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n",
    "                                      activation=tf.nn.relu) for layer in range(n_layers)]\n",
    "    multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "    outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"with_dropout\", reuse=tf.AUTO_REUSE):\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "    cells = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "             for layer in range(n_layers)]\n",
    "    keep_prob = 0.1\n",
    "    cells = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "             for cell in cells]\n",
    "    multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 50\n",
    "lstm_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GPU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_cell = tf.contrib.rnn.GPUCell(num_units = n_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"word_embedding\"):\n",
    "    vocabulary_size = 50000\n",
    "    embedding_size = 150\n",
    "    init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "    embeddings = tf.Variable(init_embeds)\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=None)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        value = [0, 1, 2, 3, 4, 5]\n",
    "        v = sess.run(embed, feed_dict={embed: value})\n",
    "        print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
