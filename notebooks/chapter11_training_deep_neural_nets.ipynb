{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope(\"inputs\", reuse=tf.AUTO_REUSE):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "        y = tf.placeholder(tf.int64, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the network\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope(\"inputs\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "            my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "            bn1 = my_batch_norm_layer(hidden1)\n",
    "            bn1_act = tf.nn.elu(bn1)\n",
    "            hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "            bn2 = my_batch_norm_layer(hidden2)\n",
    "            bn2_act = tf.nn.elu(bn1)\n",
    "            logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "            logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "with graph.as_default():\n",
    "    learning_rate = 0.01\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        # gradient clip\n",
    "#         threshold = 1.0\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#         grads_and_vars = optimizer.compute_gradients(loss)\n",
    "#         capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "#         training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define evaluation\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download data\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(images):\n",
    "    standard_scaler = StandardScaler()\n",
    "    res = standard_scaler.fit_transform(images)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = pd.read_csv(\"/Users/lgrcyanny/Codecookies/machine-learning-workspace/datasets/mnist/mnist_train.csv\", header=None)\n",
    "mnist_test = pd.read_csv(\"/Users/lgrcyanny/Codecookies/machine-learning-workspace/datasets/mnist/mnist_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_test_images = mnist_test.loc[:, 1:]\n",
    "mnist_test_labels = mnist_test.loc[:, 0]\n",
    "mnist_test_images = normalize(mnist_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size=50):\n",
    "    batch = mnist.sample(batch_size)\n",
    "    x_batch = batch.loc[:, 1:]\n",
    "    x_batch = normalize(x_batch)\n",
    "    y_batch = batch.loc[:, 0]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy_train: 0.920000016689, accuracy_test: 0.913900017738\n",
      "epoch: 1, accuracy_train: 0.939999997616, accuracy_test: 0.936699986458\n",
      "epoch: 2, accuracy_train: 0.939999997616, accuracy_test: 0.94730001688\n",
      "epoch: 3, accuracy_train: 0.980000019073, accuracy_test: 0.95609998703\n",
      "epoch: 4, accuracy_train: 0.959999978542, accuracy_test: 0.960799992085\n",
      "epoch: 5, accuracy_train: 0.959999978542, accuracy_test: 0.963599979877\n",
      "epoch: 6, accuracy_train: 0.980000019073, accuracy_test: 0.968299984932\n",
      "epoch: 7, accuracy_train: 1.0, accuracy_test: 0.968299984932\n",
      "epoch: 8, accuracy_train: 1.0, accuracy_test: 0.970499992371\n",
      "epoch: 9, accuracy_train: 1.0, accuracy_test: 0.97159999609\n",
      "epoch: 10, accuracy_train: 0.980000019073, accuracy_test: 0.973500013351\n",
      "epoch: 11, accuracy_train: 1.0, accuracy_test: 0.974600017071\n",
      "epoch: 12, accuracy_train: 1.0, accuracy_test: 0.973900020123\n",
      "epoch: 13, accuracy_train: 1.0, accuracy_test: 0.97469997406\n",
      "epoch: 14, accuracy_train: 1.0, accuracy_test: 0.975199997425\n",
      "epoch: 15, accuracy_train: 1.0, accuracy_test: 0.975499987602\n",
      "epoch: 16, accuracy_train: 1.0, accuracy_test: 0.976800024509\n",
      "epoch: 17, accuracy_train: 1.0, accuracy_test: 0.975899994373\n",
      "epoch: 18, accuracy_train: 1.0, accuracy_test: 0.97570002079\n",
      "epoch: 19, accuracy_train: 1.0, accuracy_test: 0.977800011635\n",
      "epoch: 20, accuracy_train: 1.0, accuracy_test: 0.976300001144\n",
      "epoch: 21, accuracy_train: 1.0, accuracy_test: 0.977999985218\n",
      "epoch: 22, accuracy_train: 1.0, accuracy_test: 0.980099976063\n",
      "epoch: 23, accuracy_train: 1.0, accuracy_test: 0.978200018406\n",
      "epoch: 24, accuracy_train: 0.980000019073, accuracy_test: 0.976999998093\n",
      "epoch: 25, accuracy_train: 1.0, accuracy_test: 0.980000019073\n",
      "epoch: 26, accuracy_train: 1.0, accuracy_test: 0.979099988937\n",
      "epoch: 27, accuracy_train: 1.0, accuracy_test: 0.979099988937\n",
      "epoch: 28, accuracy_train: 1.0, accuracy_test: 0.979600012302\n",
      "epoch: 29, accuracy_train: 1.0, accuracy_test: 0.980199992657\n",
      "epoch: 30, accuracy_train: 1.0, accuracy_test: 0.979099988937\n",
      "epoch: 31, accuracy_train: 1.0, accuracy_test: 0.980300009251\n",
      "epoch: 32, accuracy_train: 1.0, accuracy_test: 0.979799985886\n",
      "epoch: 33, accuracy_train: 1.0, accuracy_test: 0.979200005531\n",
      "epoch: 34, accuracy_train: 1.0, accuracy_test: 0.980099976063\n",
      "epoch: 35, accuracy_train: 1.0, accuracy_test: 0.980300009251\n",
      "epoch: 36, accuracy_train: 1.0, accuracy_test: 0.979200005531\n",
      "epoch: 37, accuracy_train: 1.0, accuracy_test: 0.980799973011\n",
      "epoch: 38, accuracy_train: 1.0, accuracy_test: 0.980599999428\n",
      "epoch: 39, accuracy_train: 1.0, accuracy_test: 0.980700016022\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "num_examples, num_columns = mnist.shape\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(num_examples // batch_size):\n",
    "#             x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            x_batch, y_batch = next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={X: x_batch, y: y_batch, training: True})\n",
    "        acc_train = sess.run(accuracy, feed_dict={X: x_batch, y: y_batch, training: True})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: mnist_test_images, y: mnist_test_labels, training: True})\n",
    "        print(\"epoch: {}, accuracy_train: {}, accuracy_test: {}\".format(epoch, acc_train, acc_test))\n",
    "    save_path = saver.save(sess, \"/tmp/tensorflow/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Reuse layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `attachment:image.png'\r\n",
      "/bin/sh: -c: line 0: `[image.png](attachment:image.png)'\r\n"
     ]
    }
   ],
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_saver = tf.train.import_meta_graph(\"/tmp/tensorflow/my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"inputs/X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"inputs/y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"train/GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "     reuse_saver.restore(sess, \"/tmp/tensorflow/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Faster Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.05  # == 1 - keep_prob\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope(\"inputs\", reuse=tf.AUTO_REUSE):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "        y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "        \n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"dnn_with_dropout\"):\n",
    "        with tf.variable_scope(\"dnn\", reuse=tf.AUTO_REUSE):\n",
    "            training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "            hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                                      name=\"hidden1\",  kernel_initializer=he_init)\n",
    "            hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "            hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                                      name=\"hidden2\")\n",
    "            hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "            logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"dropout_loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with graph.as_default():\n",
    "#     initial_learning_rate = 0.1\n",
    "#     decay_steps = 10000\n",
    "#     decay_rate = 1/10\n",
    "#     global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "#     learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "#                                                decay_steps, decay_rate)\n",
    "#     optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "#     training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "with graph.as_default():\n",
    "    learning_rate = 0.01\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy_train: 0.939999997616, accuracy_test: 0.919700026512\n",
      "epoch: 1, accuracy_train: 0.860000014305, accuracy_test: 0.937200009823\n",
      "epoch: 2, accuracy_train: 0.899999976158, accuracy_test: 0.944400012493\n",
      "epoch: 3, accuracy_train: 0.939999997616, accuracy_test: 0.949100017548\n",
      "epoch: 4, accuracy_train: 0.939999997616, accuracy_test: 0.954800009727\n",
      "epoch: 5, accuracy_train: 0.920000016689, accuracy_test: 0.957499980927\n",
      "epoch: 6, accuracy_train: 0.939999997616, accuracy_test: 0.95870000124\n",
      "epoch: 7, accuracy_train: 0.939999997616, accuracy_test: 0.962300002575\n",
      "epoch: 8, accuracy_train: 0.980000019073, accuracy_test: 0.963599979877\n",
      "epoch: 9, accuracy_train: 0.980000019073, accuracy_test: 0.966199994087\n",
      "epoch: 10, accuracy_train: 0.980000019073, accuracy_test: 0.966099977493\n",
      "epoch: 11, accuracy_train: 1.0, accuracy_test: 0.96590000391\n",
      "epoch: 12, accuracy_train: 0.980000019073, accuracy_test: 0.967100024223\n",
      "epoch: 13, accuracy_train: 1.0, accuracy_test: 0.968100011349\n",
      "epoch: 14, accuracy_train: 0.980000019073, accuracy_test: 0.968599975109\n",
      "epoch: 15, accuracy_train: 0.939999997616, accuracy_test: 0.969200015068\n",
      "epoch: 16, accuracy_train: 0.980000019073, accuracy_test: 0.969900012016\n",
      "epoch: 17, accuracy_train: 1.0, accuracy_test: 0.969699978828\n",
      "epoch: 18, accuracy_train: 0.980000019073, accuracy_test: 0.969699978828\n",
      "epoch: 19, accuracy_train: 1.0, accuracy_test: 0.971400022507\n",
      "epoch: 20, accuracy_train: 0.980000019073, accuracy_test: 0.970899999142\n",
      "epoch: 21, accuracy_train: 1.0, accuracy_test: 0.971000015736\n",
      "epoch: 22, accuracy_train: 0.980000019073, accuracy_test: 0.972699999809\n",
      "epoch: 23, accuracy_train: 0.980000019073, accuracy_test: 0.971700012684\n",
      "epoch: 24, accuracy_train: 0.980000019073, accuracy_test: 0.97310000658\n",
      "epoch: 25, accuracy_train: 1.0, accuracy_test: 0.973399996758\n",
      "epoch: 26, accuracy_train: 0.980000019073, accuracy_test: 0.972899973392\n",
      "epoch: 27, accuracy_train: 1.0, accuracy_test: 0.973399996758\n",
      "epoch: 28, accuracy_train: 0.980000019073, accuracy_test: 0.973299980164\n",
      "epoch: 29, accuracy_train: 0.980000019073, accuracy_test: 0.973599970341\n",
      "epoch: 30, accuracy_train: 0.980000019073, accuracy_test: 0.973999977112\n",
      "epoch: 31, accuracy_train: 1.0, accuracy_test: 0.974600017071\n",
      "epoch: 32, accuracy_train: 0.980000019073, accuracy_test: 0.974399983883\n",
      "epoch: 33, accuracy_train: 0.980000019073, accuracy_test: 0.974500000477\n",
      "epoch: 34, accuracy_train: 0.980000019073, accuracy_test: 0.973900020123\n",
      "epoch: 35, accuracy_train: 0.980000019073, accuracy_test: 0.975799977779\n",
      "epoch: 36, accuracy_train: 1.0, accuracy_test: 0.9742000103\n",
      "epoch: 37, accuracy_train: 1.0, accuracy_test: 0.975300014019\n",
      "epoch: 38, accuracy_train: 1.0, accuracy_test: 0.974300026894\n",
      "epoch: 39, accuracy_train: 0.980000019073, accuracy_test: 0.975000023842\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "num_examples, num_columns = mnist.shape\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(num_examples // batch_size):\n",
    "            x_batch, y_batch = next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={X: x_batch, y: y_batch, training: True})\n",
    "        acc_train = sess.run(accuracy, feed_dict={X: x_batch, y: y_batch, training: True})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: mnist_test_images, y: mnist_test_labels, training: False})\n",
    "        print(\"epoch: {}, accuracy_train: {}, accuracy_test: {}\".format(epoch, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
